{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending PyTorch differentiable functions\n",
    "\n",
    "In this notebook you'll see how to add your custom differentiable function for which you need to specify `forward` and `backward` passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "import torch\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a gentle introduction see [PyTorch extension](https://pytorch.org/docs/stable/notes/extending.html) tutorial.\n",
    "\n",
    "Source for `torch.autograd.Function` available [here](https://github.com/pytorch/pytorch/blob/master/torch/autograd/function.py).\n",
    "These are the two that we have to override:\n",
    "\n",
    "```python\n",
    "@staticmethod\n",
    "def forward(ctx, *args, **kwargs):\n",
    "    \"\"\"Performs the operation.\n",
    "    This function is to be overridden by all subclasses.\n",
    "    It must accept a context ctx as the first argument, followed by any\n",
    "    number of arguments (tensors or other types).\n",
    "    The context can be used to store tensors that can be then retrieved\n",
    "    during the backward pass.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "\n",
    "@staticmethod\n",
    "def backward(ctx, *grad_outputs):\n",
    "    \"\"\"Defines a formula for differentiating the operation.\n",
    "    This function is to be overridden by all subclasses.\n",
    "    It must accept a context :attr:`ctx` as the first argument, followed by\n",
    "    as many outputs did :func:`forward` return, and it should return as many\n",
    "    tensors, as there were inputs to :func:`forward`. Each argument is the\n",
    "    gradient w.r.t the given output, and each returned value should be the\n",
    "    gradient w.r.t. the corresponding input.\n",
    "    The context can be used to retrieve tensors saved during the forward\n",
    "    pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple\n",
    "    of booleans representing whether each input needs gradient. E.g.,\n",
    "    :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the\n",
    "    first input to :func:`forward` needs gradient computated w.r.t. the\n",
    "    output.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom addition module\n",
    "class MyAdd(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x1, x2):\n",
    "        return x1 + x2\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output, grad_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([ 0.5218, -0.8925, -0.7127], requires_grad=True)\n",
      "x2: tensor([ 0.4977, -0.0367,  0.9326], requires_grad=True)\n",
      " z: 0.10338249057531357, z.grad_fn: <MeanBackward0 object at 0x7f288c059b80>\n",
      "x1.grad: tensor([0.3333, 0.3333, 0.3333])\n",
      "x2.grad: tensor([0.3333, 0.3333, 0.3333])\n"
     ]
    }
   ],
   "source": [
    "# Let's try out the addition module\n",
    "x1 = torch.randn((3), requires_grad=True)\n",
    "x2 = torch.randn((3), requires_grad=True)\n",
    "print(f'x1: {x1}')\n",
    "print(f'x2: {x2}')\n",
    "my_add = MyAdd.apply  # aliasing the apply method\n",
    "y = my_add(x1, x2)\n",
    "z = y.mean()\n",
    "print(f' z: {z}, z.grad_fn: {z.grad_fn}')\n",
    "z.backward()\n",
    "print(f'x1.grad: {x1.grad}') # we have exactly the same contribution from each position of the vector\n",
    "# Since the output is a mean of the sum of two inputs, if we observe the gradient of the output w.r.t.\n",
    "# x1, this will be 1/length vector, and that's for each vector of the sum\n",
    "print(f'x2.grad: {x2.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom addition module\n",
    "class AddAndAverage(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x1, x2):\n",
    "        # ctx is a context where we can save\n",
    "        # computations for backward.\n",
    "        ctx.save_for_backward(x1, x2)\n",
    "        return (x1 + x2).mean()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x1, x2 = ctx.saved_tensors\n",
    "        grad_x1 = grad_output * torch.ones_like(x1) / x1.numel()\n",
    "        grad_x2 = grad_output * torch.ones_like(x2) / x2.numel()\n",
    "        # need to return grads in order \n",
    "        # of inputs to forward (excluding ctx)\n",
    "        return grad_x1, grad_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: tensor([-0.8920, -0.0960, -1.9720], requires_grad=True)\n",
      "x2: tensor([0.8276, 0.2905, 0.0565], requires_grad=True)\n",
      " z: -0.595119297504425, z.grad_fn: <torch.autograd.function.AddAndAverageBackward object at 0x7f288f9e4400>\n",
      "x1.grad: tensor([0.3333, 0.3333, 0.3333])\n",
      "x2.grad: tensor([0.3333, 0.3333, 0.3333])\n"
     ]
    }
   ],
   "source": [
    "# Let's try out the addition module\n",
    "x1 = torch.randn((3), requires_grad=True)\n",
    "x2 = torch.randn((3), requires_grad=True)\n",
    "print(f'x1: {x1}')\n",
    "print(f'x2: {x2}')\n",
    "add_and_average = AddAndAverage.apply  # aliasing the apply method\n",
    "z = add_and_average(x1, x2)\n",
    "print(f' z: {z}, z.grad_fn: {z.grad_fn}')\n",
    "z.backward()\n",
    "print(f'x1.grad: {x1.grad}')\n",
    "print(f'x2.grad: {x2.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom split module\n",
    "class MySplit(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        x1 = x.clone()\n",
    "        x2 = x.clone()\n",
    "        return x1, x2\n",
    "        \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_x1, grad_x2):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        print(f'grad_x1: {grad_x1}')\n",
    "        print(f'grad_x2: {grad_x2}')\n",
    "        return grad_x1 + grad_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x: tensor([ 0.7990, -0.1240, -1.0054, -1.0237], requires_grad=True)\n",
      "x1: tensor([ 0.7990, -0.1240, -1.0054, -1.0237], grad_fn=<MySplitBackward>)\n",
      "x2: tensor([ 0.7990, -0.1240, -1.0054, -1.0237], grad_fn=<MySplitBackward>)\n",
      " y: tensor([ 2.3969, -0.3721, -3.0162, -3.0712], grad_fn=<AddBackward0>)\n",
      " z: -1.0156515836715698, z.grad_fn: <MeanBackward0 object at 0x7f288c022280>\n",
      "grad_x1: tensor([0.2500, 0.2500, 0.2500, 0.2500])\n",
      "grad_x2: tensor([0.5000, 0.5000, 0.5000, 0.5000])\n",
      " x.grad: tensor([0.7500, 0.7500, 0.7500, 0.7500])\n"
     ]
    }
   ],
   "source": [
    "# Let's try out the split module\n",
    "x = torch.randn((4), requires_grad=True)\n",
    "print(f' x: {x}')\n",
    "split = MySplit.apply\n",
    "x1, x2 = split(x)\n",
    "print(f'x1: {x1}')\n",
    "print(f'x2: {x2}')\n",
    "y = 1 * x1 + 2 * x2\n",
    "print(f' y: {y}')\n",
    "z = y.mean()\n",
    "print(f' z: {z}, z.grad_fn: {z.grad_fn}')\n",
    "z.backward()\n",
    "print(f' x.grad: {x.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom max module\n",
    "class MyMax(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        # example where we explicitly use non-torch code\n",
    "        maximum = x.detach().numpy().max()\n",
    "        argmax = x.detach().eq(maximum).float()\n",
    "        ctx.save_for_backward(argmax)\n",
    "        return torch.tensor(maximum)\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        argmax = ctx.saved_tensors[0]\n",
    "        return grad_output * argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([ 0.0129, -0.9487, -1.0099,  0.1643, -1.3627], requires_grad=True)\n",
      "y: 0.1643255203962326, y.grad_fn: <torch.autograd.function.MyMaxBackward object at 0x7f288f9e4400>\n",
      "x.grad: tensor([0., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Let's try out the max module\n",
    "x = torch.randn((5), requires_grad=True)\n",
    "print(f'x: {x}')\n",
    "mymax = MyMax.apply\n",
    "y = mymax(x)\n",
    "print(f'y: {y}, y.grad_fn: {y.grad_fn}')\n",
    "y.backward()\n",
    "print(f'x.grad: {x.grad}') # Here only one element of the input vector contributes to the output vector, \n",
    "# so, all elements are zero except for one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
